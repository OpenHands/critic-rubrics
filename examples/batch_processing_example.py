#!/usr/bin/env python3
"""
Example demonstrating batch processing capabilities of critic_rubrics.

This example shows how to:
1. Use the new get_request() method to inspect requests
2. Process requests in batches using different providers
3. Handle both string and structured request formats
4. Use async processing for real-time requests with rate limiting
"""

from critic_rubrics import (
    create_trajectory_annotator, 
    create_solvability_annotator,
    create_batch_processor,
    BatchConfig
)



def example_string_format():
    """Example using simple string format (backward compatibility)."""
    print("=" * 60)
    print("EXAMPLE 1: String Format (Backward Compatibility)")
    print("=" * 60)
    
    annotator = create_trajectory_annotator(request_timeout=20.0)
    
    # Simple string input
    conversation = "User: Can you help me debug this Python error? Agent: Sure, what's the error?"
    
    # Get the request format without executing
    request = annotator.get_request(conversation)
    print("Generated request structure:")
    print(f"- Model: {request['model']}")
    print(f"- Messages: {len(request['messages'])} messages")
    print(f"- Tools: {len(request['tools'])} tools")
    print(f"- Tool name: {request['tools'][0]['function']['name']}")
    
    # This would work for actual annotation (but requires API key)
    # result = annotator.annotate(conversation)
    print("‚úÖ String format works correctly")


def example_structured_format():
    """Example using structured request format (like in research scripts)."""
    print("\n" + "=" * 60)
    print("EXAMPLE 2: Structured Format (Research Script Compatible)")
    print("=" * 60)
    
    annotator = create_trajectory_annotator(request_timeout=20.0)
    
    # Structured format like in the research scripts
    request_data = {
        'messages_for_annotator': [
            {
                'role': 'system', 
                'content': 'You are analyzing a conversation between a user and an AI assistant.'
            },
            {
                'role': 'user', 
                'content': 'Please analyze this conversation for trajectory issues.'
            },
            {
                'role': 'assistant',
                'content': 'I need to see the conversation to analyze it.'
            },
            {
                'role': 'user',
                'content': 'User: Fix my code\nAgent: What code?\nUser: You should know!\nAgent: I need more information.'
            }
        ],
    }
    
    # Get the request format
    request = annotator.get_request(request_data)
    print("Generated request from structured data:")
    print(f"- Uses provided messages: {len(request['messages'])} messages")
    print(f"- Uses provided tool: {request['tools'][0]['function']['name']}")
    
    # This would work for actual annotation (but requires API key)
    # result = annotator.annotate(request_data)
    print("‚úÖ Structured format works correctly")


def example_batch_processing():
    """Example of batch processing setup."""
    print("\n" + "=" * 60)
    print("EXAMPLE 3: Batch Processing Setup")
    print("=" * 60)
    
    # Create annotator and batch processor
    annotator = create_trajectory_annotator(request_timeout=20.0)
    config = BatchConfig(
        provider="openai",
        batch_size=100,
        output_folder="./batch_results"
    )
    batch_processor = create_batch_processor(annotator, config)
    
    # Example request data (like from research scripts)
    request_data_list = [
        {
            'messages_for_annotator': [
                {'role': 'user', 'content': 'Help me with Python'},
                {'role': 'assistant', 'content': 'What do you need help with?'},
                {'role': 'user', 'content': 'You should know!'}
            ],
        },
        {
            'messages_for_annotator': [
                {'role': 'user', 'content': 'Fix this error: TypeError'},
                {'role': 'assistant', 'content': 'I need to see your code'},
                {'role': 'user', 'content': 'Here it is: print(x)'}
            ],
        }
    ]
    
    # Convert to OpenAI batch format
    print("Converting to OpenAI batch format:")
    for i, request_data in enumerate(request_data_list):
        custom_id = f"conversation_{i}"
        openai_request = batch_processor.to_openai_batch_request(request_data, custom_id)
        print(f"- Request {custom_id}: {openai_request['method']} {openai_request['url']}")
        print(f"  Messages: {len(openai_request['body']['messages'])}")
        print(f"  Tools: {len(openai_request['body']['tools'])}")
    
    # Create batch file (doesn't submit to API)
    custom_ids = [f"conversation_{i}" for i in range(len(request_data_list))]
    batch_file = batch_processor.create_batch_file(request_data_list, custom_ids, "openai")
    print(f"\n‚úÖ Created batch file: {batch_file}")
    
    # Show what the batch file contains
    with open(batch_file, 'r') as f:
        lines = f.readlines()
    print(f"Batch file contains {len(lines)} requests")
    
    # Example of what batch submission would look like:
    print("\nBatch submission would work like this:")
    print("# batch_id = batch_processor.submit_batch(request_data_list, custom_ids, 'openai')")
    print("# status = batch_processor.check_batch_status(batch_id, 'openai')")
    print("# results = batch_processor.download_batch_results(batch_id, 'openai')")
    
    print("‚úÖ Batch processing setup works correctly")


def example_provider_comparison():
    """Example showing different provider formats."""
    print("\n" + "=" * 60)
    print("EXAMPLE 4: Provider Format Comparison")
    print("=" * 60)
    
    annotator = create_solvability_annotator(request_timeout=20.0)
    batch_processor = create_batch_processor(annotator)
    
    request_data = {
        'messages_for_annotator': [
            {'role': 'user', 'content': 'My app crashes when I click the button'}
        ],
    }
    
    custom_id = "issue_123"
    
    # OpenAI format
    openai_request = batch_processor.to_openai_batch_request(request_data, custom_id)
    print("OpenAI batch format:")
    print(f"- Structure: {list(openai_request.keys())}")
    print(f"- Method: {openai_request['method']}")
    print(f"- URL: {openai_request['url']}")
    print(f"- Body keys: {list(openai_request['body'].keys())}")
    
    # Anthropic format (would require anthropic package)
    print("\nAnthropic batch format:")
    try:
        # This would work if anthropic package is installed
        # anthropic_request = batch_processor.to_anthropic_batch_request(request_data, custom_id)
        # print(f"- Structure: {type(anthropic_request)}")
        print("- Would create Anthropic Request object with custom_id and params")
        print("- Uses litellm's AnthropicConfig for transformation")
    except Exception as e:
        print(f"- Requires anthropic package: {e}")
    
    print("‚úÖ Provider format comparison complete")


def main():
    """Run all examples."""
    print("CRITIC RUBRICS BATCH PROCESSING EXAMPLES")
    print("=" * 60)
    
    try:
        example_string_format()
        example_structured_format()
        example_batch_processing()
        example_provider_comparison()
        
        print("\n" + "=" * 60)
        print("üéâ ALL EXAMPLES COMPLETED SUCCESSFULLY!")
        print("=" * 60)
        print("\nKey Features Demonstrated:")
        print("‚úÖ Backward compatibility with string inputs")
        print("‚úÖ Support for structured request format from research scripts")
        print("‚úÖ Batch file creation for OpenAI and Anthropic")
        print("‚úÖ Request format inspection with get_request()")
        print("‚úÖ Provider-agnostic batch processing setup")
        
        print("\nNext Steps:")
        print("- Set API keys to enable actual LLM calls")
        print("- Use submit_batch() to send requests to providers")
        print("- Use check_batch_status() to monitor progress")
        print("- Use download_batch_results() to get parsed results")
        
    except Exception as e:
        print(f"‚ùå Error in examples: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()